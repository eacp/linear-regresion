import numpy
from sklearn.preprocessing import MinMaxScaler
from typing import List

__errors__= [];  #global variable to store the errors/loss for visualisation

def h(params, sample):
	"""This evaluates a generic linear function h(x) with current parameters.  h stands for hypothesis

	Args:
		params (lst) a list containing the corresponding parameter for each element x of the sample
		sample (lst) a list containing the values of a sample 

	Returns:
		Evaluation of h(x)
	"""
	acum = 0
	for i in range(len(params)):
		acum = acum + params[i]*sample[i]  #evaluates h(x) = a+bx1+cx2+ ... nxn.. 
	return acum;


def register_mse(params: List[float] , samples,ys: List[float]):
	"""Appends the errors/loss that are generated by the estimated values of h and the real value y
	
	Args:
		params (lst) a list containing the corresponding parameter for each element x of the sample
		samples (lst) a 2 dimensional list containing the input samples 
		y (lst) a list containing the corresponding real result for each sample
	
	"""
	global __errors__
	acc = 0
#	print("transposed samples") 
#	print(samples)
	for i in range(len(samples)):
		hyp = h(params,samples[i])
		error=hyp-ys[i]
		acc=+error**2 # this error is the original cost function, (the one used to make updates in GD is the derivated verssion of this formula)
	mean_error_param=acc/len(samples)
	__errors__.append(mean_error_param)







def GD(params, samples, y, alfa: float = 0.1):
	"""Gradient Descent algorithm 
	Args:
		params (lst) a list containing the corresponding parameter for each element x of the sample
		samples (lst) a 2 dimensional list containing the input samples 
		y (lst) a list containing the corresponding real result for each sample
		alfa(float) the learning rate
	Returns:
		temp(lst) a list with the new values for the parameters after 1 run of the sample set
	"""
	new_params = list(params)

	for j in range(len(params)):
		acc =0; 
		for i in range(len(samples)):
			error = h(params,samples[i]) - y[i]

			#Sumatory part of the Gradient Descent formula for linear Regression.
			# Just like the formula

			acc += error*samples[i][j]  

		# Continuation of the formula
		new_params[j] = params[j] - alfa*(1/len(samples))*acc  
		
	return new_params



#  multivariate example
params = [0,0,0]
samples = [[1,1],[2,2],[3,3],[4,4],[5,5],[2,2],[3,3],[4,4]]
y = [2,4,6,8,10,2,5.5,16]

alfa =.01  #  learning rate
for i in range(len(samples)):
	if isinstance(samples[i], list):
		samples[i]=  [1]+samples[i]
	else:
		samples[i]=  [1,samples[i]]
print ("original samples:")
print (samples)

# Create a scaler
scaler = MinMaxScaler()
scaler.fit(samples)
samples = scaler.transform(samples)
print ("scaled samples:")
print (samples)


epochs = 200

ep = 0

while ep < epochs:  #  run gradient descent until local minima is reached
	oldparams = list(params)

	params=GD(params, samples,y,alfa)	

	# register the MSE for later analysis and visualisation
	register_mse(params, samples, y)  
	# print (params)

	ep += 1

# Show the computed params
print("This are the resulting params from the training: ", params)

import matplotlib.pyplot as plt  #use this to generate a graph of the errors/loss so we can see whats going on (diagnostics)
plt.plot(__errors__)
plt.show()